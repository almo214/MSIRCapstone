{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:Maroon;\">\n",
    "   \n",
    "<br>    \n",
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://www.troy.edu/\" target=\"_blank\">\n",
    "    <img src=\"https://www.troy.edu/_resources/images/troy-logo-svg.svg\" width=\"200\" alt=\"Troy University Logo\"  /> \n",
    "    </a>\n",
    "</p>\n",
    "<br> \n",
    "<p style=\"text-align:center\">\n",
    "<h1 style=\"color:White;line-height:.2\"><center>MSIR Capstone </center> </h1>\n",
    "<h3 style=\"color:White;line-height:.2\"><center>Allison Moore</center> </h3>\n",
    "<h2 style=\"color:White;line-height:.8\"><center>October 2022</center> </h2>  \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:Black;\">\n",
    "    <h6> </h6>   \n",
    "<h3 style=\"color:White;line-height:1.3\"\"><center>Administrative Tasks & Data Wrangling </center> </h3>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to normalize a column of data\n",
    "def normal(col):\n",
    "    temp=[]\n",
    "    least=np.min(col)\n",
    "    most=np.max(col)\n",
    "    for i in col:\n",
    "        n=((i)-least)/(most-least)\n",
    "        temp.append(n)\n",
    "    return temp\n",
    "\n",
    "#Define a function to normalize an entire DataFrame\n",
    "def normal_DF(df):\n",
    "    normalized_df=df\n",
    "    for colName, colValue in df.iteritems():\n",
    "        normalized_df[colName]=normal(colValue)\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate a confusion matrix \n",
    "# to identify True Positives, True Negatives, False Positives,\n",
    "# and False Negatives during Machine Learning\n",
    "\n",
    "def plot_confusion_matrix(y,y_predict):\n",
    "    \"this function plots the confusion matrix\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y, y_predict)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['0', '1', '2', '3', '4', '5', '6']); ax.yaxis.set_ticklabels(['0', '1', '2', '3', '4', '5', '6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two DataFrames will be utilized. Both will contain the same data.\n",
    "# However, one DataFrame will utilize only numerical, normalized values\n",
    "# while the second will maintain some categorical text, for ease of \n",
    "# understanding during Data Visualization.\n",
    "\n",
    "# Read the numerical CSV file\n",
    "df=pd.read_csv('Numerical_Data.csv')\n",
    "\n",
    "#Normalize the numerical Data Frame\n",
    "ndf=normal_DF(ndf)\n",
    "\n",
    "#Average each variable across each unique value of Severity.\n",
    "Temp=ndf.groupby('SEV').mean()\n",
    "ind=pd.Series([0,1,2,3,4,5,6])\n",
    "#Reindex the new DataFrame with the values of the Severity variable.\n",
    "Avg_ndf=Temp.set_index(ind)\n",
    "Avg_ndf['SEV']=Temp.index\n",
    "\n",
    "\n",
    "# Read the multi-type categorical CSV file\n",
    "cdf=pd.read_csv('Categorical_Data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:Black;\">\n",
    "    <h4> </h4>   \n",
    "<h1 style=\"color:White;line-height:1.2\"\"><center>Exploratory Data Analysis </center> </h1>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality correlation matrix\n",
    "k = 8 #number of variables for heatmap\n",
    "cols = df.corr().nlargest(k, 'SEV')['SEV'].index\n",
    "cm = df[cols].corr()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm, annot=True, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliers\n",
    "#Use on Normalized DF\n",
    "\n",
    "l = df.columns.values\n",
    "number_of_columns=8\n",
    "number_of_rows = len(l)-1/number_of_columns\n",
    "plt.figure(figsize=(number_of_columns,5*number_of_rows))\n",
    "for i in range(0,len(l)):\n",
    "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.boxplot(df[l[i]],color='green',orient='v')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewness\n",
    "plt.figure(figsize=(2*number_of_columns,5*number_of_rows))\n",
    "for i in range(0,len(l)):\n",
    "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
    "    sns.distplot(df[l[i]],kde=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:Black;\">\n",
    "    <h4> </h4>   \n",
    "<h1 style=\"color:White;line-height:1.2\"\"><center> Data Visualization </center> </h1>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the averaged olity for each value of severity\n",
    "sns.catplot(x=\"PVV\", y='SEV', hue=\"SEV\", data=Avg_ndf, aspect = 2)\n",
    "plt.ylabel(\"Severity\",fontsize=20)\n",
    "plt.xlabel(\"Polity\",fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:Black;\">\n",
    "    <h4> </h4>   \n",
    "<h1 style=\"color:White;line-height:1.2\"\"><center> Machine Learning </center> </h1>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=ndf[['PVV','INT','METH','ICT','DPPL','CPL','GFP']]\n",
    "y=ndf['SEV']\n",
    "transform = preprocessing.StandardScaler(x)\n",
    "X=transform\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\n",
    "lr=LogisticRegression()\n",
    "\n",
    "logreg_cv = GridSearchCV(lr, parameters,cv=10, iid=None)\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n",
    "TA_Log= logreg_cv.best_estimator_.score(X_test, y_test)\n",
    "print(TA_Log)\n",
    "\n",
    "yhat=logreg_cv.predict(X_test)\n",
    "plot_confusion_matrix(y_test,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "parameters = {'criterion': ['gini', 'entropy'],\n",
    "     'splitter': ['best', 'random'],\n",
    "     'max_depth': [2*n for n in range(1,10)],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv=GridSearchCV(tree,parameters, cv=10)\n",
    "tree_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\n",
    "print(\"accuracy :\",tree_cv.best_score_)\n",
    "TA_Tree=tree_cv.best_estimator_.score(X_test, y_test)\n",
    "print(TA_Tree)\n",
    "y_tree = tree_cv.predict(X_test)\n",
    "plot_confusion_matrix(y_test,y_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),\n",
    "              'C': np.logspace(-3, 3, 5),\n",
    "              'gamma':np.logspace(-3, 3, 5)}\n",
    "svm = SVC()\n",
    "\n",
    "svm_cv=GridSearchCV(svm, parameters, cv=10)\n",
    "svm_cv.fit(X_train, Y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\n",
    "print(\"accuracy :\",svm_cv.best_score_)\n",
    "TA_SVM= svm_cv.best_estimator_.score(X_train, Y_train)\n",
    "print(TA_SVM)\n",
    "y_svm=svm_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,y_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Nearest Neighbors\n",
    "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'p': [1,2]}\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "knn_cv=GridSearchCV(KNN, parameters, cv=10)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\n",
    "print(\"accuracy :\",knn_cv.best_score_)\n",
    "TA_KNN=knn_cv.best_estimator_.score(X_test, y_test)\n",
    "print(TA_KNN)\n",
    "y_Knn = knn_cv.predict(X_test)\n",
    "plot_confusion_matrix(y_test,y_Knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Model Accuracies to determine the best fit for the data\n",
    "score={'SVM':TA_SVM, 'KNN': TA_KNN,'Decision Tree' :TA_Tree,'Logistic Regression': TA_Log}\n",
    "scores=pd.DataFrame(data=score, index=[0])\n",
    "scores.head()\n",
    "scores.plot(kind='bar', figsize=(20, 15))\n",
    "\n",
    "plt.xlabel('Model',fontsize=20)\n",
    "plt.ylabel('Accuracy',fontsize=20) \n",
    "plt.title('Model Prediciton Accuracy',fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
